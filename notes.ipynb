{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Markov Process**\n",
    "**Markov Property:** State $S_{t}$ is Markov if and only if\n",
    "$$ P[S_{t+1} | S_{t}] = P[S_{t+1} | S_{1}, S_{2}, ..., S_{t}] $$\n",
    "\n",
    "**State transition matrix:** Gives the probability of going from one state $S$ to $S~{'}$\n",
    "$$ P_{SS^{'}} = P[S_{t+1} = s^{'} | S_{t} = s]  $$\n",
    "This above function is represented as a matrix which defines the dynamics of the Markov process\n",
    "$$ P = \\begin{bmatrix}P_{11} & ... & P_{1n}\\\\.\\\\.\\\\. \\\\P_{n1}&...& P_{nn}\\end{bmatrix} $$\n",
    "\n",
    "---\n",
    "\n",
    "**Markov Process:** can be defined by a tuple $\\langle S, P \\rangle $ Where,  \n",
    "$S$ - is a finite set of all states  \n",
    "$P$ - is the transition matrix  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Markov Process Example**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Markov Reward Process**\n",
    "---\n",
    "\n",
    "Can be defined by a tuple $\\langle S, P, R, \\gamma \\rangle$ Where,  \n",
    "$S$ - is a finite set of all states  \n",
    "$P$ - is the transition dynamics  \n",
    "$R$ - is the reward function. It is the reward you get when you exit a state $S$ [note: for time steps, $R$ is indexed $(t+1)$ if $S$ is indexed $t$]\n",
    "$$ R_{s} = E[R_{t+1} | S_{t} = s]$$\n",
    "$\\gamma$ - is the discount factor  \n",
    "\n",
    "---\n",
    "\n",
    "Total discounted reward for time-step $t$  \n",
    "$$ G_{t} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} + ... = \\sum_{k=0}^{\\infty}\\gamma^{k} R_{t+k+1} $$\n",
    "Discounting is done mainly to have mathematical convenience and there is a lot of uncertainity in the future compared to the present\n",
    "\n",
    "#### **Value function**\n",
    "The state value function of a Markov reward process is the expected return starting from the state $S$\n",
    "$$ v(s)  = E[G_{t} | S_{t} = s] $$\n",
    "\n",
    "#### **Using Bellman Equation for value function**\n",
    "Consists of two parts  \n",
    "1. Immediate reward $R_{t+1}$ and   \n",
    "2. Discounted value of successor state $\\gamma v(S_{t+1})$\n",
    "\n",
    "$$ v(s) = E[G_{t} | S_{t}=s] $$\n",
    "$$ v(s) = E[R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} + ... | S_{t}=s] $$\n",
    "$$ v(s) = E[R_{t+1} + \\gamma G_{t+1} | S_{t}=s] $$\n",
    "$$ v(s) = E[R_{t+1} + \\gamma v(S_{t+1}) | S_{t}=s] $$  \n",
    "\n",
    "$$ v(s) = R_{s} + \\gamma \\sum_{s^{'}\\epsilon S} P_{SS^{'}}v(S^{'}) $$\n",
    "\n",
    "#### **Bellman Equation in Matrix form**\n",
    "$$ \\begin{bmatrix}v(1) \\\\ . \\\\.\\\\.\\\\v(n) \\end{bmatrix} = \\begin{bmatrix}R_{1} \\\\ . \\\\.\\\\.\\\\R_{n} \\end{bmatrix}  +  \\gamma \\begin{bmatrix}P_{11} & ... & P_{1n}\\\\.\\\\.\\\\. \\\\P_{n1}&...& P_{nn}\\end{bmatrix}   \\begin{bmatrix}v(1) \\\\ . \\\\.\\\\.\\\\v(n) \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Markov Decision Process**\n",
    "---\n",
    "\n",
    "Markov decision process is Markove reward process with actions or decisions.  \n",
    "Can be defined by a tuple $\\langle S, A, P, R, \\gamma \\rangle$ Where,  \n",
    "$S$ - is a finite set of all states  \n",
    "$A$ - is a finite set of actions  \n",
    "$P$ - is the transition dynamics  \n",
    "$$ P_{SS^{'}}^{a} = P[S_{t+1} = s~{'} | S_{t} = s, A_{t} = a]  \\end{equation*}\n",
    "note: This is to imply one separate transcition matrix for each action  \n",
    "$R$ - is the reward function. It is the reward you get when you exit a state $S$ [note: for time steps, $R$ is indexed $(t+1)$ if $S$ is indexed $t$]\n",
    "$$ R_{s}^{a} = E[R_{t+1} | S_{t} = s, A_{t}=a]\\end{equation*}\n",
    "$\\gamma$ - is the discount factor [0,1]  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Policy function**\n",
    "A policy is a distribution over actions given state.\n",
    "$$ \\pi(a|s) = P[A_{t}=a | S_{t}=s] \\end{equation*}\n",
    "Policies are not time dependent. A policy is same no matter what time step.\n",
    "\n",
    "#### **Extracting MRP and MP from MDP**\n",
    "Given an MDP $\\langle S, A, P, R, \\gamma \\rangle$ and a policy $\\pi$  \n",
    "Any state sequence $S_{1},S_{2},...$ is a Markov process $\\langle S,P^{\\pi}\\rangle$  \n",
    "The state and reward sequence $S_{1},R_{2}, S_{2}, R_{3},...$ is a Markov process $\\langle S,P^{\\pi},R^{\\pi},\\gamma \\rangle$ where,  \n",
    "$$ P^{\\pi}_{S,S^{'}} = \\sum_{a\\epsilon A} \\pi(a|s)P_{S,S^{'}}^{a} \\end{equation*}\n",
    "$$ R_{S}^{\\pi} = \\sum_{a\\epsilon A} \\pi(a|s)R_{S}^{a} \\end{equation*}\n",
    "\n",
    "\n",
    "#### **Value function**\n",
    "**State value function:** $v_{\\pi}(s)$ of an MDP is the expected return starting from state $s$ and following policy $\\pi$\n",
    "$$ v_{\\pi}(s) = E_{\\pi}[G_{t} | S_{t}=s] \\end{equation*}\n",
    "\n",
    "**Policy value function:** $q_{\\pi}(s,a)$ of an MDP is the expected return starting from state $s$, taking action $a$ and following policy $\\pi$\n",
    "$$ q_{\\pi}(s,a) = E_{\\pi}[G_{t} | S_{t}=s,A_{t}=a] \\end{equation*}\n",
    "\n",
    "Also, to write v and q in terms of each other,  \n",
    "$$ v_{\\pi}(s) = \\sum_{a \\epsilon A} \\pi(a|s)q_{\\pi}(s,a) \\end{equation*}\n",
    "$$ q_{\\pi}(s,a) = R_{s}^{a} + \\gamma \\sum_{s^{'} \\epsilon S} P_{SS^{'}}^{a}v_{\\pi}(s^{'}) \\end{equation*}\n",
    "By, putting them together we get,\n",
    "$$ v_{\\pi}(s) = \\sum_{a \\epsilon A}\\pi(a|s)(R_{s}^{a} + \\gamma \\sum_{s^{'} \\epsilon S} P_{SS^{'}}^{a}v_{\\pi}(s^{'})) \\end{equation*}\n",
    "$$ q_{\\pi}(s,a) = R_{s}^{a} + \\gamma \\sum_{s^{'} \\epsilon S} P_{SS^{'}}^{a}\\sum_{a^{'} \\epsilon A} \\pi(a^{'}|s^{'})q_{\\pi}(s^{'},a) \\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "#### **Using Bellman equation for State & Policy value functions**\n",
    "Both the State and Policy value functions can be decomposed into immediate reward plus discounted value for successor state.\n",
    "$$ v_{\\pi}(s) = E_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1})| S_{t}=s] $$\n",
    "$$ q_{\\pi}(s,a) = E_{\\pi}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_{t}=s,A_{t}=a] $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Optimality in MDP**\n",
    "\n",
    "#### **Optimal value functions**\n",
    "Optimal state value function $v_{*}(s)$ is the maximum value function over all policies.\n",
    "\\begin{equation*}\n",
    "v_{*}(s) = max_{\\pi}v_{\\pi}(s) \n",
    "\\end{equation*}\n",
    "Optimal action value function $q_{*}(s,a)$ is the maximum action value function over all policies.\n",
    "\\begin{equation*} q_{*}(s,a) = max_{\\pi}q_{\\pi}(s,a) \\end{equation*}\n",
    "\n",
    "#### **Optimal policy**\n",
    "Any policy  is said to $\\pi$ be better than $\\pi^{'}$ if the value of all states by following $\\pi$ is better than that by following $\\pi^{'}$  \n",
    "$ \\pi \\geq \\pi^{'} $ if $v_{\\pi}(s) \\geq v_{\\pi^{'}}(s)$ for all $s$  \n",
    "**Theorem:**\n",
    "1. There exists an optimal policy that is better than or equal all other policies $\\pi_{*} \\geq \\pi$, for all $\\pi$\n",
    "2. All optimal policies achieve the optimal value function $v_{\\pi_{*}}(s) = v_{*}(s)$\n",
    "3. All optimal policies achieve the optimal action value function $q_{\\pi_{*}}(s,a) = q_{*}(s,a)$\n",
    "\n",
    "#### **Finding the optimal policy**\n",
    "An optimal policy can be found by maximising over $q_{*}(s,a)$\n",
    "\\begin{equation*} \n",
    "\\pi_{*}(a|s) = 1 if a=argmax_{a\\epsilon A} q_{*}(s,a)\n",
    "\\end{equation*}\n",
    "\n",
    "#### **Bellman Optimality equation**\n",
    "Optimal value functions are recursively related by the Bellman optimality equations.\n",
    "The optimality for state value is the maximum of action value that can be taken. Choose the action which gives the highest value.\n",
    "\\begin{equation*}v_{*}(s) = max_{a}q_{*}(s,a)\\end{equation*}\n",
    "The optimality for action value is the sum of immediate reward and average over all the states that occur after taking an action. We don't control what happens after taking an action. We will end up in some new state with given transition probabilities.\n",
    "\\begin{equation*}q_{*}(s,a) = R_{s}^{a} + \\gamma \\sum_{s^{'} \\epsilon S}P_{SS^{'}}^{a}v_{*}(s^{'})\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
