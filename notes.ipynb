{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Markov Process**\n",
    "**Markov Property:** State $S_{t}$ is Markov if and only if\n",
    "$$ P[S_{t+1} | S_{t}] = P[S_{t+1} | S_{1}, S_{2}, ..., S_{t}] $$\n",
    "\n",
    "**State transition matrix:** Gives the probability of going from one state $S$ to $S^{'}$\n",
    "$$ P_{SS^{'}} = P[S_{t+1} = s^{'} | S_{t} = s]  $$\n",
    "This above function is represented as a matrix which defines the dynamics of the Markov process\n",
    "$$ P = \\begin{bmatrix}P_{11} & ... & P_{1n}\\\\.\\\\.\\\\. \\\\P_{n1}&...& P_{nn}\\end{bmatrix} $$\n",
    "\n",
    "---\n",
    "\n",
    "**Markov Process:** can be defined by a tuple $\\langle S, P \\rangle $ Where,  \n",
    "$S$ - is a finite set of all states  \n",
    "$P$ - is the transition matrix  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Markov Process Example**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Markov Reward Process**\n",
    "---\n",
    "\n",
    "Can be defined by a tuple $\\langle S, P, R, \\gamma \\rangle$ Where,  \n",
    "$S$ - is a finite set of all states  \n",
    "$P$ - is the transition dynamics  \n",
    "$R$ - is the reward function. It is the reward you get when you exit a state $S$ [note: for time steps, $R$ is indexed $(t+1)$ if $S$ is indexed $t$]\n",
    "$$ R_{s} = E[R_{t+1} | S_{t} = s]$$\n",
    "$\\gamma$ - is the discount factor  \n",
    "\n",
    "---\n",
    "\n",
    "Total discounted reward for time-step $t$  \n",
    "$$ G_{t} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} + ... = \\sum_{k=0}^{\\infty}\\gamma^{k} R_{t+k+1} $$\n",
    "Discounting is done mainly to have mathematical convenience and there is a lot of uncertainity in the future compared to the present\n",
    "\n",
    "#### **Value function**\n",
    "The state value function of a Markov reward process is the expected return starting from the state $S$\n",
    "$$ v(s)  = E[G_{t} | S_{t} = s] $$\n",
    "\n",
    "#### **Using Bellman Equation for value function**\n",
    "Consists of two parts  \n",
    "1. Immediate reward $R_{t+1}$ and   \n",
    "2. Discounted value of successor state $\\gamma v(S_{t+1})$\n",
    "\n",
    "$$ v(s) = E[G_{t} | S_{t}=s] $$\n",
    "$$ v(s) = E[R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} + ... | S_{t}=s] $$\n",
    "$$ v(s) = E[R_{t+1} + \\gamma G_{t+1} | S_{t}=s] $$\n",
    "$$ v(s) = E[R_{t+1} + \\gamma v(S_{t+1}) | S_{t}=s] $$  \n",
    "\n",
    "$$ v(s) = R_{s} + \\gamma \\sum_{s^{'}\\epsilon S} P_{SS^{'}}v(S^{'}) $$\n",
    "\n",
    "#### **Bellman Equation in Matrix form**\n",
    "$$ \\begin{bmatrix}v(1) \\\\ . \\\\.\\\\.\\\\v(n) \\end{bmatrix} = \\begin{bmatrix}R_{1} \\\\ . \\\\.\\\\.\\\\R_{n} \\end{bmatrix}  +  \\gamma \\begin{bmatrix}P_{11} & ... & P_{1n}\\\\.\\\\.\\\\. \\\\P_{n1}&...& P_{nn}\\end{bmatrix}   \\begin{bmatrix}v(1) \\\\ . \\\\.\\\\.\\\\v(n) \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Markov Decision Process**\n",
    "---\n",
    "\n",
    "Markov decision process is Markov reward process with actions or decisions.  \n",
    "Can be defined by a tuple $\\langle S, A, P, R, \\gamma \\rangle$ Where,  \n",
    "$S$ - is a finite set of all states  \n",
    "$A$ - is a finite set of actions  \n",
    "$P$ - is the transition dynamics  \n",
    "$$ P_{SS^{'}}^{a} = P[S_{t+1} = s~{'} | S_{t} = s, A_{t} = a]  $$\n",
    "note: This is to imply one separate transcition matrix for each action  \n",
    "$R$ - is the reward function. It is the reward you get when you exit a state $S$ [note: for time steps, $R$ is indexed $(t+1)$ if $S$ is indexed $t$]\n",
    "$$ R_{s}^{a} = E[R_{t+1} | S_{t} = s, A_{t}=a] $$\n",
    "$\\gamma$ - is the discount factor [0,1]  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Policy function**\n",
    "A policy is a distribution over actions given state.\n",
    "$$ \\pi(a|s) = P[A_{t}=a | S_{t}=s] $$\n",
    "Policies are not time dependent. A policy is same no matter what time step.\n",
    "\n",
    "#### **Extracting MRP and MP from MDP**\n",
    "Given an MDP $\\langle S, A, P, R, \\gamma \\rangle$ and a policy $\\pi$  \n",
    "Any state sequence $S_{1},S_{2},...$ is a Markov process $\\langle S,P^{\\pi}\\rangle$  \n",
    "The state and reward sequence $S_{1},R_{2}, S_{2}, R_{3},...$ is a Markov process $\\langle S,P^{\\pi},R^{\\pi},\\gamma \\rangle$ where,  \n",
    "$$ P^{\\pi}_{S,S^{'}} = \\sum_{a\\epsilon A} \\pi(a|s)P_{S,S^{'}}^{a} $$\n",
    "$$ R_{S}^{\\pi} = \\sum_{a\\epsilon A} \\pi(a|s)R_{S}^{a} $$\n",
    "\n",
    "\n",
    "#### **Using Bellman equation for State & Policy value functions**\n",
    "Both the State and Policy value functions can be decomposed into immediate reward plus discounted value for successor state.\n",
    "$$ v_{\\pi}(s) = E_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1})| S_{t}=s] $$\n",
    "$$ q_{\\pi}(s,a) = E_{\\pi}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_{t}=s,A_{t}=a] $$\n",
    "\n",
    "\n",
    "#### **Value function**\n",
    "**State value function:** $v_{\\pi}(s)$ of an MDP is the expected return starting from state $s$ and following policy $\\pi$\n",
    "$$ v_{\\pi}(s) = E_{\\pi}[G_{t} | S_{t}=s] $$\n",
    "\n",
    "**Policy value function:** $q_{\\pi}(s,a)$ of an MDP is the expected return starting from state $s$, taking action $a$ and following policy $\\pi$\n",
    "$$ q_{\\pi}(s,a) = E_{\\pi}[G_{t} | S_{t}=s,A_{t}=a] $$\n",
    "\n",
    "Also, to write v and q in terms of each other,  \n",
    "$$ v_{\\pi}(s) = \\sum_{a \\epsilon A} \\pi(a|s)q_{\\pi}(s,a) $$\n",
    "$$ q_{\\pi}(s,a) = R_{s}^{a} + \\gamma \\sum_{s^{'} \\epsilon S} P_{SS^{'}}^{a}v_{\\pi}(s^{'}) $$\n",
    "By putting them together we get,\n",
    "$$ v_{\\pi}(s) = \\sum_{a \\epsilon A}\\pi(a|s)(R_{s}^{a} + \\gamma \\sum_{s^{'} \\epsilon S} P_{SS^{'}}^{a}v_{\\pi}(s^{'})) $$\n",
    "$$ q_{\\pi}(s,a) = R_{s}^{a} + \\gamma \\sum_{s^{'} \\epsilon S} P_{SS^{'}}^{a}\\sum_{a^{'} \\epsilon A} \\pi(a^{'}|s^{'})q_{\\pi}(s^{'},a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Optimality in MDP**\n",
    "\n",
    "#### **Optimal value functions**\n",
    "Optimal state value function $v_{*}(s)$ is the maximum value function over all policies.\n",
    "$$ v_{*}(s) = max_{\\pi}v_{\\pi}(s) $$\n",
    "Optimal action value function $q_{*}(s,a)$ is the maximum action value function over all policies.\n",
    "$$ q_{*}(s,a) = max_{\\pi}q_{\\pi}(s,a) $$\n",
    "\n",
    "#### **Optimal policy**\n",
    "Any policy $\\pi$ is said to be better than $\\pi^{'}$ if the value of all states by following $\\pi$ is better than that by following $\\pi^{'}$  \n",
    "$ \\pi \\geq \\pi^{'} $ if $v_{\\pi}(s) \\geq v_{\\pi^{'}}(s)$ for all $s$  \n",
    "**Theorem:**\n",
    "1. There exists an optimal policy that is better than or equal all other policies $\\pi_{*} \\geq \\pi$, for all $\\pi$\n",
    "2. All optimal policies achieve the optimal value function $v_{\\pi_{*}}(s) = v_{*}(s)$\n",
    "3. All optimal policies achieve the optimal action value function $q_{\\pi_{*}}(s,a) = q_{*}(s,a)$\n",
    "\n",
    "#### **Finding the optimal policy**\n",
    "An optimal policy can be found by maximising over $q_{*}(s,a)$\n",
    "$$ \\pi_{*}(a|s) = 1 if a=argmax_{a\\epsilon A} q_{*}(s,a) $$\n",
    "\n",
    "#### **Bellman Optimality equation**\n",
    "Optimal value functions are recursively related by the Bellman optimality equations.\n",
    "The optimality for state value is the maximum of action value that can be taken. Choose the action which gives the highest value.\n",
    "$$ v_{*}(s) = max_{a}q_{*}(s,a) $$\n",
    "The optimality for action value is the sum of immediate reward and average over all the states that occur after taking an action. We don't control what happens after taking an action. We will end up in some new state with given transition probabilities.\n",
    "$$ q_{*}(s,a) = R_{s}^{a} + \\gamma \\sum_{s^{'} \\epsilon S}P_{SS^{'}}^{a}v_{*}(s^{'}) $$\n",
    "By putting them together we get,\n",
    "$$ v_{*}(s) = max_{a}(R_{s}^{a} + \\gamma \\sum_{s^{'} \\epsilon S}P_{SS^{'}}^{a}v_{*}(s^{'})) $$\n",
    "$$ q_{*}(s,a) = R_{s}^{a} + \\gamma \\sum_{s^{'} \\epsilon S}P_{SS^{'}}^{a}max_{a^{'}}q_{*}(s^{'},a^{'}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Solving the MDP**\n",
    "#### **Planning problem**\n",
    "We are given the full knowledge of MDP (both the dynamics & rewards) and we solve that MDP using Dynamic programming.\n",
    "\n",
    "##### **Prediction problem**:  \n",
    "The policy evaluation phase. How much reward or value do we get by following a policy.  \n",
    "Input: The complete MDP $\\langle S, A, P, R, \\gamma \\rangle$ and a policy $\\pi$  \n",
    "Output: The value function $v_{\\pi}$\n",
    "\n",
    "Problem: Evaluate a given policy  \n",
    "Solution:\n",
    "1. Start with initial value function $v_{0}$ which is typically initialized with zeros, follow a policy $\\pi$ and sweep all states repeat till you converge to $v_{\\pi}$\n",
    "2. Each iteration uses Bellman expectation equation to update value for the next iteration until the values converge.  \n",
    "For $k=0,1,2,3...$ Following $\\pi$ till the value function converges ($v_{\\pi}$)\n",
    "$$ v_{k+1}(s) = \\sum_{a \\epsilon A}\\pi(a|s)(R_{s}^{a} + \\gamma \\sum_{s^{'} \\epsilon S} P_{SS^{'}}^{a}v_{k}(s^{'})) $$\n",
    "\n",
    "##### **Control problem**:  \n",
    "Given an MDP, of all the policies what is the most reward that can be achieved in the MDP?\n",
    "Input: The complete MDP $\\langle S, A, P, R, \\gamma \\rangle$  \n",
    "Output: The optimal value function $v_{*}$ and of course the optimal policy $\\pi_{*}$\n",
    "\n",
    "**Type 1. Policy Iteration**  \n",
    "Problem: Find optimal value function and optimal policy given MDP.  \n",
    "Solution: Start with a policy $\\pi$  \n",
    "Iteratively do the following until you reach the optimal policy iteration  \n",
    "1. Evaluate the policy using above method\n",
    "$$ v_{\\pi}(s) = E[R_{t+1} + \\gamma R_{t+2} + ... | S_{t}=s] $$\n",
    "2. Improve the policy by acting greedily with respect to the value $v_{\\pi}$\n",
    "$$ \\pi^{'} = greedy(v_{\\pi}) $$ \n",
    "Which implies new policy is taking the action with maximum return given a state. If we keep doing that we reach Bellman optimality equation.\n",
    "$$ \\pi^{'} = argmax_{a\\epsilon A} q_{\\pi}(s,a) $$\n",
    "\n",
    "Evaluating policy needs sweeping of all states, this can be expensive. Also, in some cases, Policy converges to optimal even before the value function is computed. All further computation after the policy convergence is pointless. There are some ways we can improve this.\n",
    "1. We can introduce a stopping condition (e.g $\\epsilon convergence$ to value function)  \n",
    "2. Simply stop and update policy greedily after $k$ iterations of iterative policy evaluation. Note here that $k$ is the from prediction problem, we don't continue evaluating policy till $v_{\\pi}$ is reached, we stop for a set $k$ iterations.\n",
    "3. We can also, update policy with each of the evaluation iteration (same as above with $k=1$). This is value iteration in the next section\n",
    "\n",
    "**Type 2. Value Iteration**  \n",
    "Principle of Optimality: A policy $\\pi(s|a)$ achieves the optimal value from state $s$, $v_{\\pi}(s)$ = $v_{*}(s)$, if and only if, for any state $s^{'}$ reachable from $s$, $\\pi$ achieves the optimal value from state $s^{'}$, $v_{\\pi}(s^{'})$ = $v_{*}(s^{'})$\n",
    "\n",
    "If we know the solution for optimality for subproblems $v_{*}(s^{'})$. Then, $v_{*}(s)$ can be found with one step lookahead. Just like prediction probelm, we do this iteratively using Bellman optimality equation instead of Bellman expectation equation and update value for next iteration until we reach the optimality.\n",
    "$$ v_{k+1}(s) = max_{a}(R_{s}^{a} + \\gamma \\sum_{s^{'} \\epsilon S}P_{SS^{'}}^{a}v_{k}(s^{'})) $$\n",
    "\n",
    "Here is an [excellent demo](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html) for both Policy and Value iteration\n",
    "\n",
    "**Summary**  \n",
    "\n",
    "|Problem    |Bellman Equation                                         |Algorithm                   |\n",
    "|:----------|:--------------------------------------------------------|:---------------------------|\n",
    "|Prediction |Bellman Expectation Equation                             |Iterative Policy Evaluation |\n",
    "|Control    |Bellman Expectation Equation + Greedy Policy improvement |Policy Iteration            |\n",
    "|Control    |Bellman Optimality Equation                              |Value Iteration             |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model-Free Prediction**\n",
    "\n",
    "#### **Monte-Carlo Policy evaluation**\n",
    "The procedure is to sample many episodes (state -> terminate). For each episode, the value of a state is calculated as below\n",
    "$$ G_{t} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} + ... = \\sum_{k=0}^{\\infty}\\gamma^{k} R_{t+k+1} $$\n",
    "$$ v(s)  = E[G_{t} | S_{t} = s] $$\n",
    "This value of the state is averaged over all the episodes to get the value function of a particular state. There are two flavours of averaging, one is to use only first visit to a state in an episode. Another, is to use all the visits to a state in an episode.\n",
    "\n",
    "The mean computation need not be done all at once, we can use incremental mean.\n",
    "$$\\mu_{k} = \\frac{1}{k} \\sum_{j=1}^{k}x_{j}$$\n",
    "By rearranging, it can also be written as\n",
    "$$\\mu_{k} = \\mu_{k-1} + \\frac{1}{k}(x_{k} - \\mu_{k-1})$$\n",
    "This takes the form of, new_mean = old_mean + (step_size * offset_value).\n",
    "\n",
    "Using this, we can re-write the Monte-Carlo value function as.\n",
    "For each state $S_{t}$ with return $G_{t}$,\n",
    "$$ N(S_{t}) := N(S_{t}) + 1 $$\n",
    "$$ v(S_{t}) := v(S_{t}) + \\frac{1}{N(S_{t})}(G_{t} - v(S_{t})$$\n",
    "Instead of always using a mean, if we want to forget old episodes like in the case of a non-stationary problem then a more general form would be\n",
    "$$ v(S_{t}) := v(S_{t}) + \\alpha(G_{t} - v(S_{t}))$$\n",
    "Where, $\\alpha$ is fixed step size and can be tuned as we wish.\n",
    "\n",
    "#### **Temporal Difference Learning (TD-Learning)**\n",
    "Temporan difference is just like Monte-Carlo, excpet we don't traverse the whole episode to update the value of a state, we update the value of a state using estimate of the value of next state. That is, we change the $G_{t}$ to $ [R_{t+1} + \\gamma v(S_{t+1})]$\n",
    "$$ v(S_{t}) := v(S_{t}) + \\alpha(R_{t+1} + \\gamma v(S_{t+1}) - v(S_{t}))$$\n",
    "#### **TD($\\lambda$)-Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
